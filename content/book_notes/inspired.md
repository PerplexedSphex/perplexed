---
title: "Inspired"
date: 2018-09-02T18:08:27-07:00
---

> At one level, the responsibilities of the product manager are pretty straightforward. He or she is responsible for evaluating opportunities and determining what gets built and delivered to customers. We generally describe what needs to get built on the product backlog.
                
> The product is the result of what the product team builds, and the product manager is responsible for what the product team will build. So, this is why the product manager is the person we hold responsible and accountable for the success of the product. When a product succeeds, it's because everyone on the team did what they needed to do. But when a product fails, it's the product manager's fault.
                
> there are four key responsibilities of a strong product manager;
                
> First and foremost is deep knowledge of the actual users and customers.
                
> Today, product managers are expected to be comfortable with data and analytics.
                
> The third critical contribution—and the one that is often considered the most difficult by many product managers—is a deep understanding of your business and how it works, and the role your product plays in your business. This is tougher than it sounds.
                
> This means knowing who your various stakeholders are and especially learning the constraints they operate under. There are usually key stakeholders representing general management, sales, marketing, finance, legal, business development, and customer service. Your CEO is usually a very important stakeholder as well.
                
> Succeeding in the job of product means convincing each key stakeholder that you understand their constraints and that you are committed to only delivering solutions that you believe are consistent with those constraints.
                
> The fourth critical contribution is deep knowledge of the market and industry in which you're competing.
                
> To summarize, these are the four critical contributions you need to bring to your team: deep knowledge (1) of your customer, (2) of the data, (3) of your business and its stakeholders, and (4) of your market and industry.
                
> The successful product manager must be the very best versions of smart, creative, and persistent.
                
> Perhaps the most important thing I can tell you to help you succeed is that you simply must take very seriously your preparation for this role. Start by becoming an expert in your users and customers. Share very openly what you learn, both the good and the bad. Become your team's and your company's go‐to person for understanding anything about your customer—quantitative and qualitative. Work to establish a strong relationship with your key stakeholders and business partners. Convince them of two things: (1) You understand the constraints they operate under. (2) You will only bring to them solutions that you believe will work within those constraints. Become an undisputed expert on your product and your industry. Again, share your knowledge openly and generously. Finally, work very hard to build and nurture the strong collaborative relationship with your product team. I'm not saying that doing all this is easy; it's not.
                
> In the old model, designers took requirements or specifications from product managers and used that to create their designs. In contrast, modern product designers continuously collaborate with product managers and engineers—from discovery to delivery.
                
> Rather than being measured on the output of their design work, the product designer is measured on the success of the product. Given this, product designers have many of the same concerns as product managers.
                
> User experience (UX) is much bigger than user interface (UI). Some people even use the term customer experience to further emphasize the point. UX is any way that customers and end users realize the value provided by your product. It includes all the touch points and interactions a customer has with your company and product over time. For modern products, this usually includes multiple different interfaces, as well as other customer touch points (e‐mail, marketing campaigns, sales process, customer support, and so forth).
                
> Depending on the product, the list of touch points could be very long, considering questions as: How will customers first learn about the product? How will we onboard a first‐time user and (perhaps gradually) reveal new functionality? How might users interact at different times during their day? What other things are competing for the user's attention? How might things be different for a one‐month‐old customer versus a one‐year‐old customer? How will we motivate a user to a higher level of commitment to the product? How will we create moments of gratification? How will a user share his experience with others? How will customers receive an offline service? What is the perceived responsiveness of the product?
                
> Good product designers use prototypes as their primary canvas for communicating ideas, both internally and externally.
                
> roles. Interaction design generally includes the underlying conceptual models (e.g., a photo management application may have photos, albums, projects), task flows, and control layouts to manipulate those concepts. Visual design includes composition, typography, and how the visual brand is expressed. Modern product designers may have different strengths but, generally, have some level of skill with both interaction and visual design.
                
> In strong teams today, the design informs the functionality at least as much as the functionality drives the design. This is a hugely important concept. For this to happen, we need to make design a first‐class member of the product team, sitting side by side with the product manager, and not a supporting service.
                
> Once you get a designer dedicated to your product team, here are five keys to a successful and healthy relationship with your designer: Do whatever you need to do to have your designer sit next to you. Include your designer from the very inception of every idea. Include your designer in as many customer and user interactions as possible. Learn about the users and customers together. Fight your temptation to provide your designer with your own design ideas. Give your designer as much room as possible to solve the design challenges him or herself. Encourage your designer to iterate early and often. The best way you can encourage this is to not get all nitpicky about design details with the very early iterations. More generally, encourage your designer to feel free not to just iterate on the particular design approach but to explore alternative solutions to the problem.
                
> There's probably no more important relationship for a successful product manager than the one with your engineers. If your relationship is strong, with mutual and sincere respect both ways, then the product manager job is great. If your relationship is not strong, your days as product manager will be brutal (and probably numbered).
                
> This strong relationship begins with you. You need to do your homework and bring to the team the knowledge and skills of good product management.
                
> As a practical matter, you need to engage directly with your engineers every workday. There are typically two types of discussions going on each day. In the first type of discussion, you're soliciting their ideas and input for the items you're working on in discovery. In the second type of discussion, they're asking you clarifying questions on the items they're working on delivering to production.
                
> Where a lot of product managers go sideways is in how they communicate with their engineers. Just as most product managers don't like it when an executive or stakeholder spells out exactly what they want you to build, engineers generally don't like it when you try to spell out how to build something. So, while it's good if you have a strong technology understanding, it's not good if you use that knowledge to try to do their jobs for them.
                
> One last thing to keep in mind: the morale of the engineers is very much a function of you as the product manager. It is your job to make sure they feel like missionaries and not mercenaries. You do this by involving them deeply in the customer pain you are trying to solve and in the business problems you face. Don't try to shelter them from this—instead, share these problems and challenges very openly with them. They will respect you more for it, and, in most cases, the developers will rise to the challenge.
                
> In general, from the product management perspective, any senior engineer is helpful because of the broad knowledge he or she brings that pertains to what is possible. However, a tech lead not only has this knowledge—and is responsible for helping to share this knowledge with the other engineers on the team—but the tech lead also has an explicit responsibility to help the product manager and product designer discover a strong solution.
                
> Product marketing is most typically organized by customer‐facing product, by target market, or sometimes by go‐to‐market channel, especially for more established companies (e.g., enterprise, vertical, mid‐market). There are typically fewer product marketers than product teams, as such, they get spread across different product teams.
                
> As you'll soon see, coming up with winning products is never easy. We need a product that our customers love, yet also works for our business. However, a very large component of what is meant by works for our business is that there is a real market there (large enough to sustain a business), we can successfully differentiate from the many competitors out there, we can cost‐effectively acquire and engage new customers, and we have the go‐to‐market channels and capabilities required to get our product into the hands of our customers.
                
> Modern product marketing managers represent the market to the product team—the positioning, the messaging, and a winning go‐to‐market plan. They are deeply engaged with the sales channel and know their capabilities, limitations, and current competitive issues.
                
> One of the key themes of this book is focusing on outcome and not output. Realize that typical product roadmaps are all about output. Yet, good teams are asked to deliver business results.
                
> I define product roadmap as a prioritized list of features and projects your team has been asked to work on.
                
> In some cases, product roadmaps come down from management (usually referred to as a stakeholder‐driven roadmap) and sometimes the roadmap comes from the product manager. They don't usually include little things like bugs and optimizations, but they do normally contain the requested features, projects, and big, multi‐team efforts often called initiatives. And they typically include due dates or at least time frames for when each item is expected to be delivered.
                
> Management has fair reasons for wanting product roadmaps: First, they want to be sure you're working on the highest‐value things first. Second, they are trying to run a business, which means they need to be able to plan. They want to know when key capabilities will launch so they can coordinate marketing programs, sales force hiring, dependencies with partners, and so on. These are reasonable desires. Yet, typical roadmaps are the root cause of most waste and failed efforts in product organizations.
                
> Even with the best of intentions, product roadmaps typically lead to very poor business results. I refer to the reasons for this as the two inconvenient truths about product. The first inconvenient truth is that at least half of our product ideas are just not going to work. There are many reasons for a product idea to not pan out.
                
> If that's not bad enough, the second inconvenient truth is that, even with the ideas that do prove to be valuable, usable, feasible, and viable, it typically takes several iterations to get the execution of this idea to the point where it delivers the expected business value that management was hoping for. This is often referred to as time to money.
                
> Weak teams just plod through the roadmap they've been assigned, month after month. And, when something doesn't work—which is often—first they blame it on the stakeholder that requested/demanded the feature and then they try to schedule another iteration on the roadmap, or they suggest a redesign or a different set of features that this time they hope will solve the problem.
                
> In contrast, strong product teams understand these truths and embrace them rather than deny them. They are very good at quickly tackling the risks (no matter where that idea originated) and are fast at iterating to an effective solution. This is what product discovery is all about, and it is why I view product discovery as the most important core competency of a product organization. If we can prototype and test ideas with users, customers, engineers, and business stakeholders in hours and days—rather than in weeks and months—it changes the dynamics, and most important, the results.
                
> It's worth pointing out that it isn't the list of ideas on the roadmap that's the problem. If it was just ideas, there's not much harm in that. The issue is that anytime you put a list of ideas on a document entitled “roadmap,” no matter how many disclaimers you put on it, people across the company will interpret the items as a commitment. And that's the crux of the problem, because now you're committed to building and delivering this thing, even when it doesn't solve the underlying problem. Don't misinterpret this. Sometimes, we do need to commit to a delivery on a date. We try to minimize those cases, but there are always some. But we need to make what is called a high‐integrity commitment.
                
> Before we jump into the alternative, however, we need to remind ourselves that roadmaps have existed for so long because they serve two purposes, and these needs don't go away: The first purpose is because the management of the company wants to make sure that teams are working on the highest‐business‐value items first. The second purpose is because—since they're trying to run a business—there are cases where they need to make date‐based commitments, and the roadmap is where they see and track those commitments (even though in most companies, they rarely trust the dates anymore).
                
> In the empowered product team model this book is predicated on, the teams are themselves equipped to figure out the best ways to solve the particular business problems assigned to them. But for this to happen, it's not enough to have strong people equipped with modern tools and techniques. The product teams need to have the necessary business context.
                
> For technology companies, there are two main components that provide this business context: The product vision and strategy. This describes the big picture of what the organization as a whole is trying to accomplish and what the plan is for achieving that vision. Each of our product teams may have its own areas of focus (for example, buyer teams and seller teams), but it's all supposed to come together to achieve the product vision. The business objectives. This describes the specific, prioritized business objectives for each product team.
                
> That's a good example of a business objective for one or more product teams: “Dramatically reduce the time it takes for a new customer to go live.” And one of the measurable key results would be “Average new customer onboarding time less than three hours.”
                
> The second driver is the occasional need for committing to a hard date. We address this with the concept of high‐integrity commitments, used for those situations where we need to commit to a date or a specific deliverable.
                
> First, the teams are much more motivated when they are free to solve the problem the best way they see fit. It's the missionary versus mercenary thing again.
                
> Second, the team is not off the hook just by delivering a requested feature or project. The feature must solve the business problem (as measured by the key results);
                
> Third, no matter where the idea for the solution comes from, or how smart that person is, very often the initial approach doesn't work out.
                
> It is all about outcome rather than output.
                
> There are a few product teams out there that have modified their product roadmaps so that each item is stated as a business problem to solve rather than the feature or project that may or may not solve it. These are called outcome‐based roadmaps.
                
> In most Agile teams, when you even mention the word “commitments” (like knowing what you're going to launch and when it will happen), you get reactions ranging from squirming to denial. It's a constant struggle between those executives and stakeholders who are trying to run the business (with hiring plans, marketing program spend, partnerships, and contracts depending on specific dates and deliverables) and the product team that is understandably reluctant to commit to dates and deliverables. They're reluctant when they don't yet understand what they need to deliver, and if it will work in terms of delivering the necessary business results, in addition to not knowing how much it will really cost because they don't yet know the solution.
                
> The key is to understand that the root cause of all this grief about commitments is when these commitments are made. They are made too early. They are made before we know whether we can deliver on this obligation, and even more important, whether what we deliver will solve the problem for the customer.
                
> We ask the executives and our other stakeholders to give us a little time in product discovery to investigate the necessary solution. We need the time to validate that solution with customers to ensure it has the necessary value and usability, with engineers to ensure its feasibility, and with our stakeholders to ensure it is viable for our business. Once we have come up with a solution that works for our business, we now can make an informed and high‐integrity commitment about when we can deliver and what business results we can expect. Note that our delivery managers are key to determining any commitment dates. Just because your engineers believe something might take only two weeks to build and deliver, what if that team is already occupied on other work, and they can't start on this work for another month? The delivery managers track these commitments and dependencies.
                
> The product vision describes the future we are trying to create, typically somewhere between two and five years out.
                
> Note also that the vision is not in any sense a spec. It's mainly a persuasive piece that might be in the form of a storyboard, a narrative such as a white paper, or a special type of prototype referred to as a visiontype.
                
> Its primary purpose is to communicate this vision and inspire the teams (and stakeholders, investors, partners—and, in many cases, prospective customers) to want to help make this vision a reality.
                
> The product strategy is our sequence of products or releases we plan to deliver on the path to realizing the product vision.
                
> For most types of businesses, I encourage teams to construct product strategy around a series of product/market fits. There are many variations on this (the strategy for the product strategy, if you will). For business‐focused companies, you might have each product/market fit focus on a different vertical market (e.g., financial services, manufacturing, automotive).
                
> There's no single approach to product strategy that is ideal for everyone, and you can never know how things might have gone if you sequenced your product work differently. I tell teams that the most important benefit is just that you decided to focus your product work on a single target market at a time. So, all teams know we're tackling the manufacturing market now, and that's the type of customers we are obsessing on. Our goal is to come up with the smallest actual deliverable product that makes these manufacturing customers successful. Ideas that come up that pertain to other types of customers or markets are saved for future consideration.
                
> For a product team to be empowered and act with any meaningful degree of autonomy, the team must have a deep understanding of the broader context. This starts with a clear and compelling product vision, and the path to achieving that vision is the product strategy. The more product teams you have, the more essential it is to have this unifying vision and strategy for each team to be able to make good choices. And, just to be clear—the idea is not that every product team has its own product vision. That would miss the point. The idea is that our organization has a product vision, and all the product teams in that organization are helping to contribute to making that vision a reality.
                
> The difference between vision and strategy is analogous to the difference between good leadership and good management. Leadership inspires and sets the direction, and management helps get us there. Most important, the product vision should be inspiring, and the product strategy should be focused.
                
> In terms of prioritizing markets, all I said above was to prioritize your markets and focus on them one at a time. I didn't say how to prioritize them. There is no one right way to do this, but there are three critical inputs to your decision:
                
> The first is market sizing, usually referred to as total addressable market (TAM).
                
> The second factor concerns distribution, usually referred to as go to market (GTM).
                
> The third factor is a (very rough) estimation of how long it will take, referred to as time to market (TTM).
                
> These are the 10 key principles for coming up with an effective product vision.
                
> Start with why.
                
> Fall in love with the problem, not with the solution.
                
> Don't be afraid to think big with vision.
                
> Don't be afraid to disrupt yourselves because, if you don't, someone else will.
                
> The product vision needs to inspire.
                
> Determine and embrace relevant and meaningful trends.
                
> Skate to where the puck is heading, not to where it was.
                
> Be stubborn on vision but flexible on the details.
                
> Realize that any product vision is a leap of faith.
                
> Evangelize continuously and relentlessly.
                
> As we discussed previously, there are any number of approaches to product strategy, but good strategies have these five principles in common:
                
> Focus on one target market or persona at a time.
                
> Product strategy needs to be aligned with business strategy.
                
> Product strategy needs to be aligned with sales and go‐to‐market strategy.
                
> Obsess over customers, not over competitors.
                
> Communicate the strategy across the organization.
                
> Where the product vision describes the future you want to create, and the product strategy describes your path to achieving that vision, the product principles speak to the nature of the products you want to create.
                
> The MBO system was refined and improved at several companies over the years, most notably by the legendary Andy Grove at Intel. Today, the primary business objective management system we use is known as the OKR system—objectives and key results.
                
> The concept is straightforward and based on two fundamental principles:
                
> The first can easily be summed up with the famous General George Patton quote I mentioned earlier: “Never tell people how to do things. Tell them what to do, and they will surprise you with their ingenuity.” The second was captured by HP's tagline of that era: “When performance is measured by results.” The idea here is that you can release all the features you want, but if it doesn't solve the underlying business problem, you haven't really solved anything.
                
> While there are several workable systems and tools for managing these business objectives, in this book, I'll focus on the OKR system technique.
                
> The Objectives and Key Results (OKR) technique is a tool for management, focus, and alignment.
                
> Objectives should be qualitative; key results need to be quantitative/measurable. Key results should be a measure of business results, not output or tasks. The rest of the company will use OKRs a bit differently, but for the product management, design, and technology organization, focus on the organization's objectives and the objectives for each product team, which are designed to roll up and achieve the organization's objectives. Don't let personal objectives or functional team objectives dilute or confuse the focus. Find a good cadence for your organization (typically, annually for an organization's objectives and quarterly for a team's objectives). Key results should be a measure of business results, not output or tasks. Keep the number of objectives and key results for the organization and for each team small (one to three objectives, with one to three key results each is typical). It's critical that every product team track their active progress against their objectives (which is typically weekly). The objectives do not need to cover every little thing the team does, but they should cover what the team needs to accomplish.
                
> It's important that, one way or another, teams feel accountable to achieving their objectives. If they fail substantially, it's worth having a post‐mortem/retrospective with some of their peers or management. Agree as an organization on how you will be evaluating or scoring your key results. There are different approaches to this, and it's in large part a reflection of your particular company culture. What's important here is consistency across the organization, so that teams know when they can depend on one another. It's common to define a score of 0 (on a scale from 0 to 1.0) if you essentially make no progress, 0.3 if you just did the bare minimum—what you know you can achieve, 0.7 if you've accomplished more than the minimum and have really done what you'd hoped you would achieve, and 1.0 if you've really surprised yourselves and others with a truly exceptional result, beyond what people were even hoping for. Establish very clear and consistent ways to indicate when a key result is in reality a high‐integrity commitment (described earlier) rather than a normal objective. In other words, for most key results, you may be shooting for that 0.7 score. But for a high‐integrity commitment, these are special, and it's more binary. You either delivered what you promised or you didn't. Be very transparent (across the product and technology organization) on what objectives each product team is working on and their current progress. Senior management (CEO and executive team) is responsible for the organization's objectives and key…

> If you deploy OKRs for your product organization, the key is to focus your OKRs at the product team level. This means don't let functional team or individual person OKRs confuse the issue. Focus the attention of the individuals on their product team objectives. If different functional organizations (such as design, engineering, or quality assurance) have larger objectives (such as responsive design, technical debt, and test automation), they should be discussed and prioritized at the leadership team level along with the other business objectives, and then incorporated into the relevant product team's objectives.
                
> The key is that the cascading of OKRs in a product organization needs to be up from the cross‐functional product teams to the company or business‐unit level.
                
> If you're a product manager—especially at a large company—and you're not good at evangelism, there's a very strong chance that your product efforts will get derailed before they see the light of day. And even if product does manage to ship, it will likely go the way of thousands of other large company efforts and wither on the vine.
                
> There are several techniques to help communicate the value of what you're proposing to your team, colleagues, stakeholders, executives, and investors. Here are my top‐10 pieces of advice for product managers to sell the dream:
                
> Use a prototype.
                
> Share the pain. Show the team the customer pain you are addressing. This is why I love to bring engineers along for customer visits and meetings.
                
> Share the vision. Make sure you have a very clear understanding of your product vision, product strategy, and product principles. Show how your work contributes to this vision and is true to the principles.
                
> Share learnings generously. After every user test or customer visit, share your learnings—not just the things that went well, but share the problems, too.
                
> Share credit generously. Make sure the team views it as their product, not just your product. However, when things don't go well, step forward and take responsibility for the miss and show the team you're learning from the mistakes as well. They'll respect you for it.
                
> Learn how to give a great demo. This is an especially important skill to use with customers and key execs. We're not trying to teach them how to operate the product, and we're not trying to do a user test on them. We're trying to show them the value of what we're building. A demo is not training, and it's not a test. It's a persuasive tool. Get really, really good at it.
                
> Do your homework. Your team and your stakeholders will all be much more likely to follow you if they believe you know what you're talking about.
                
> Be genuinely excited. If you're not excited about your product, you should probably fix that—either by changing what you work on or by changing your role.
                
> Learn to show some enthusiasm. Assuming you're genuinely excited, it's amazing to me how many product managers are so bad or so uncomfortable at showing enthusiasm. This matters—a lot.
                
> Spend time with your team. If you're not spending significant face time with your designer and every engineer on your team, then they can't see the enthusiasm in your eyes.
                
> For most teams, there are two very significant challenges to tackle. First, discovering in detail what the customer solution needs to be. That includes everything from making sure there are enough customers that even need this solution (the demand) and then coming up with a solution that works for our customers and for our business. Even harder, we need to make sure we come up with a single solution that works for many customers, and not a series of specials. To do this, we need to be able to test out many ideas, and we need to do this quickly and inexpensively. Second, we need to ensure we deliver a robust and scalable implementation that our customers can depend on for consistently reliable value. Your team needs to be able to release with confidence. While you'll never have 100 percent confidence, you should not have to release and pray. So, we need to learn fast, yet also release with confidence.
                
> Many teams get into a lot of grief with the concept of a minimum viable product (MVP) because on the one hand we are very motivated to get this out in front of customers fast to get feedback and learn. And, on the other hand, when we do get out there fast, people feel like this so‐called product is an embarrassment to the brand and the company. How could we possibly consider launching this?
                
> In general, I find that most product teams have a much better sense of how to accomplish the second goal of delivering solid software than how to accomplish the first goal of rapid experimentation and discovery.
                
> Part of what causes confusion is a dilution of what is really meant when we call something a “product” or “product‐quality” or “productized” or “live in production.” I always try hard to reserve the term product to describe the state at which we can run a business on it. Specifically, it is scalable and performant to the degree necessary. It has a strong suite of automated regression tests. It is instrumented to collect the necessary analytics. It has been internationalized and localized where appropriate. It is maintainable. It is consistent with the brand promise. And, most important, it is something the team can release with confidence.
                
> Doing all this work when the product manager isn't even sure this is the solution the customer wants or needs is a recipe for product failure and big waste. So, the purpose of product discovery is to make sure we have some evidence that when we ask the engineers to build a production‐quality product, it won't be a wasted effort. And, this is why we have so many different techniques in product discovery.
                
> And, in fact, most of the techniques don't require the developer's time (which is important, because we appreciate how much time and effort needs to go into creating production‐quality software in delivery). Much of the key to effective product discovery is getting access to our customers without trying to push our quick experiments into production.
                
> But here's the key. If you want to discover great products, it really is essential that you get your ideas in front of real users and customers early and often. If you want to deliver great products, you want to use best practices for engineering and try not to override the engineers' concerns.
                
> The purpose of product discovery is to address these critical risks: Will the customer buy this, or choose to use it? (Value risk) Can the user figure out how to use it? (Usability risk) Can we build it? (Feasibility risk) Does this solution work for our business? (Business viability risk) And it's not enough that it's just the product manager's opinion on these questions. We need to collect evidence.
                
> When it comes to how we do product discovery, there are a set of core principles that drive how we work.
                
> We know we can't count on our customers (or our executives or stakeholders) to tell us what to build.
                
> The most important thing is to establish compelling value. It's all hard, but the hardest part of all is creating the necessary value so that customers ultimately choose to buy or to use.
                
> As hard and important as the engineering is, coming up with a good user experience is usually even harder, and more critical to success.
                
> Functionality, design, and technology are inherently intertwined.
                
> We expect that many of our ideas won't work out, and the ones that do will require several iterations.
                
> We must validate our ideas on real users and customers. One of the most common traps in product is to believe that we can anticipate our customer's actual response to our products.
                
> Our goal in discovery is to validate our ideas the fastest, cheapest way possible. Discovery is about the need for speed. This lets us try out many ideas, and for the promising ideas, try out multiple approaches.
                
> We need to validate the feasibility of our ideas during discovery, not after. If the first time your developers see an idea is at sprint planning, you have failed. We need to ensure the feasibility before we decide to build, not after.
                
> We need to validate the business viability of our ideas during discovery, not after. Similarly, it is absolutely critical to ensure that the solution we build will meet the needs of our business—before we take the time and expense to build out that product.
                
> It's about shared learning. One of the keys to having a team of missionaries rather than a team of mercenaries is that the team has learned together.
                
> Most product teams normally think of an iteration as a delivery activity. If you release weekly, you think in terms of one‐week iterations. But we also have the concept of an iteration in discovery. We loosely define an iteration in discovery as trying out at least one new idea or approach. It's true that ideas come in all shapes and sizes, and some are much riskier than others, but the purpose of discovery is to do this much faster and cheaper than we can do in delivery. To set your expectations, teams competent in modern discovery techniques can generally test on the order of 10–20 iterations per week.
                
> As a rule of thumb, an iteration in discovery should be at least an order of magnitude less time and effort than an iteration in delivery.
                
> Much of our product discovery work doesn't require a lot of framing or planning. We need to come up with a solution to a particular problem, and often this is straightforward, and we can proceed directly to delivery work. But for many efforts, this is decidedly not the case, and some framing and true problem solving becomes critically important. Big projects—and, especially, initiatives (projects spanning multiple teams)—are common examples. In this section, I consider how we frame our discovery work to ensure alignment and to identify key risks. There are really two goals here:
                
> The first is to ensure the team is all on the same page in terms of clarity of purpose and alignment. In particular, we need to agree on the business objective we're focused on, the specific problem we are intending to solve for our customers, which user or customers you're solving that problem for, and how you will know if you've succeeded. These should align directly to your product team's objectives and key results. The second purpose is to identify the big risks that will need to be tackled during the discovery work. I find that most teams tend to gravitate toward a particular type of risk that they are most comfortable with.
                
> In this section, I describe three of my favorite techniques, each for different‐sized efforts:
                
> An opportunity assessment is designed for the vast majority of product work,
                
> A customer letter is designed for larger projects or initiatives that often have multiple goals
                
> A startup canvas for those times you're creating an entirely new product line or a new business.
                
> Note that these techniques are not mutually exclusive.
                
> But one of the most important lessons in our industry is to fall in love with the problem, not the solution. Why is this so important? Because, more often than not, our initial solutions don't solve the problem—at least not in a way that can power a successful business.
                
> An opportunity assessment is an extremely simple technique but can save you a lot of time and grief.
                
> The idea is to answer four key questions about the discovery work you are about to undertake: What business objective is this work intended to address? (Objective) How will you know if you've succeeded? (Key results) What problem will this solve for our customers? (Customer problem) What type of customer are we focused on? (Target market)
                
> For smaller and more typically sized efforts, the opportunity assessment is usually sufficient. But when embarking on a somewhat larger effort, there may in fact be multiple reasons, several customer problems to be solved, or business objectives to be tackled. To communicate the value effectively, it may take more than the four questions listed in the previous chapter. A typical example of an effort of this size would be a redesign. There are likely several objectives in the redesign, and maybe it is intended to both improve the experience for current customers and perform better for new customers.
                
> The idea is that rather than communicate the benefits in a press release format, you describe them in the format of a customer letter written from the hypothetical perspective of one of your product's well‐defined user or customer personas. The letter—sent to the CEO from a very happy and impressed customer—explains why he or she is so happy and grateful for the new product or redesign. The customer describes how it has changed or improved his or her life. The letter also includes an imagined congratulatory response from the CEO to the product team explaining how this has helped the business.
                
> In this situation, you have a much broader set of risks, including validating your value proposition, figuring out how you intend to make money, how you plan to get this product out to your customers and sell to them, how much it will cost to produce and sell this product, and what you will measure to track your progress—not to mention determining whether the market is large enough to sustain a business.
                
> I much prefer the startup canvas to old‐style business plans, but I have also observed that many startup teams still spend too much time on the canvas and keep postponing that pesky little problem of discovering a solution that people want to buy (see the box “The Biggest Risk”).
                
> One of the things I like about a startup canvas is that it helps to quickly highlight the key assumptions and major risks facing a startup or a significant new product in an existing business. This is a good thing. The idea is to tackle the biggest risks first. At least that's the theory. In practice, I keep running into entrepreneurs and product leaders who are focused on secondary risks rather than primary risks.
                
> Look, if you can discover a solution that your customers love, then you can tackle the risks of monetization and scale. However, without that solution, the rest of your work is very likely going to be wasted. So, whether your constrained resource is cash or management's patience, you need to make sure you primarily use your time to discover a winning solution. Get that risk resolved first and then you can focus on the other risks. The point is that you don't need to spend your time doing pricing optimization testing, sales tools, marketing programs, and cutting costs, until and unless you have discovered a truly valuable product.
                
> In this section, I describe two of my favorite discovery‐planning techniques. One is simple (story maps), and the other is fairly complicated (customer discovery program), but they are both remarkably powerful and effective.
                
> Story maps are one of the most generally useful techniques I know.
                
> The origin of story maps came from frustration with the typical flat backlog of user stories. There's no context, just a prioritized list of stories.
                
> These are two‐dimensional maps, in which major user activities are arrayed along the horizontal dimension, loosely ordered by time from left to right. So, if there are a dozen major user activities, they would be along the top from left to right, generally in the order you would do them—or at least, if you were describing the overall system to someone else, the order in which you'd describe them.
                
> Along the vertical dimension, we have a progressive level of detail. As we flesh out each major activity into sets of user tasks, we add stories for each of those tasks. The critical tasks are higher vertically than the optional tasks.
                
> Now each story has context. The entire team can see how it fits in with the other stories. And not just as a snapshot in time. The team can see how the system is expected to grow over time. We can use this story map to frame our prototypes, and then as we get feedback on our prototypes and learn how people interact with our product ideas, we can easily update the story map to serve as a living reflection of the prototypes. As we finalize our discovery work and progress into delivery, the stories from the map move right into the product backlog.
                
> Many teams I know consider a high‐fidelity user prototype and a story map as their go‐to techniques.
                
> Another must‐read book for product managers: User Story Mapping: Discover the Whole Story, Build the Right Product, by Jeff Patton (O'Reilly Media, 2014).
                
> Let's be clear about what it means to be a reference customer: This is a real customer (not friends or family), who is running your product in production (not a trial or prototype), who has paid real money for the product (it wasn't given away to entice them to use it), and, most important, who is willing to tell others how much they love your product (voluntarily and sincerely).
                
> Please believe me when I say that there are few things more powerful to a product organization than reference customers. It is the single best sales tool you can provide to your sales and marketing organization, and it completely changes the dynamics between the product organization and the rest of the company. Ask any good salesperson the single best tool you can provide to help her do her job, and she'll say, “happy reference customers.”
                
> The reason I love the customer discovery program technique so much is because it is designed to produce these reference customers. We are discovering and developing a set of reference customers in parallel with discovering and developing the actual product.
                
> I will warn you that this technique takes substantial effort, primarily on the part of the product manager. I wish it were easier. But I will also say that if you do this technique, I consider it the single best leading indicator of future product success.
                
> There are four main variations of this technique for four different situations: Building products for businesses Building platform products (e.g., public APIs) Building customer‐enabling tools used by employees of your company Building products for consumers
                
> I also need to point out that you would not do this program for small efforts like features or minor projects. This is for larger efforts. Good examples would be creating a new product or business, taking an existing product to a new market or new geography or a redesign of a product.
                
> For products and services aimed at businesses, I was taught years ago that the key number is six reference customers.
                
> Now these are not just any six customers. We are looking to develop six reference customers in our specific target market or segment, so, the idea is to find six similar customers. If you end up targeting two or three customers from two or three different markets, this program will not give you the focus you want and need.
                
> We want to end with six reference customers, so we'll typically recruit between six and eight in case one or two turn out to be not a match or unavailable. We need them to be from the specific target market we are going after. They may be from your existing customer base, prospects, or a blend.
                
> However, it's also important we screen out technologists. These people are mainly interested because of the technology, not because they desperately need the business value.
                
> We need them to have people and time willing to work closely with us. They need to be willing to spend time with the product team, testing out early prototypes and helping the team ensure the product works well for them. If possible, we would like them to be well‐recognized marquee names, because that will be of the most value to the sales and marketing staff.
                
> The benefit to the prospective customer is that they get real input, not lip service, to the solution—and, most important, they get a solution that truly works for them.
                
> The benefit to the product team is that you get ready access to a set of users and customers that you can go deep with and figure out a solution that will work for them. They've provided you access to their users. They have agreed to test early versions. And, what's really important, they have agreed to buy the product and serve as a public reference if the resulting product works for them.
                
> It's critical to explain to each prospective member of the program that your job is to come up with a general product—something your company can successfully sell to a large number of customers. You're not trying to build a custom solution that only works for that one company (and they wouldn't want that in any case as they would be left with unsupported, dead‐end software). You are, however, deeply committed to coming up with a product that works extremely well for them and just a handful of other companies.
                
> Further, your job as product manager is not to put in the features that all six companies request. While that would be much easier, that would yield an awful product. Your job is to dive deep with each of the six customers and identify a single solution that works well for all six customers.
                
> You need to make sure your customers are truly from your target market and not more than one target market. A big benefit of this program is focus, and that means the customers are from a single target market. You will want to work with your product marketing manager to ensure that the prospective customer has permission from their marketing organization to serve as a public reference. You will also want to keep your product marketing partner continuously involved in this program as she can help turn your reference customer into some great sales tools and collateral. But, remember, it is your job to develop those actual reference customers—so be sure you deliver a product they love.
                
> I like this customer discovery program so much is that I consider this a very practical and very effective definition of product/market fit. If we can get to the point where we have six reference customers in a specific target market, we will typically declare product/market fit for that market. Remember product/market fit does not mean that you are done with working on that product. Not even close. We will continue to improve that product continuously for years. However, once we have those six reference customers, we can aggressively and effectively sell that product to other customers in that market. So, each reference customer is a truly significant milestone. But, for example, getting six reference customers in a given target market for a B2B company is perhaps the most significant, meaningful milestone business result for a product organization and something truly worth celebrating.
                
> There are, of course, any number of techniques for generating product ideas. I really haven't met many ideation techniques I haven't liked. But to me, the more relevant question is, “How do we generate the types of ideas that are likely to truly help us solve the hard business problems that our leaders have asked us to focus on right now?”
                
> The customer interview is the most basic technique I'll discuss in this book.
                
> There are many forms of customer interviews, so this is not really a single technique. Some are informal and some are more formal. Some have a user research methodology behind them (one of my favorites is the contextual inquiry), and others are more about just getting out of the building and learning what you don't know.
                
> But in every user or customer interaction, we always have the opportunity to learn some valuable insights. Here's what I'm always trying to understand: Are your customers who you think they are? Do they really have the problems you think they have? How does the customer solve this problem today? What would be required for them to switch?
                
> lead. Here are some tips for getting the most out of these learning opportunities: Frequency. Establish a regular cadence of customer interviews. This should not be a once‐in‐a‐while thing. A bare minimum would be two to three hours of customer interviews per week, every week. Purpose. You are not trying to prove anything during these interviews, one way or the other. You're just trying to understand and learn quickly. This mindset is critical and needs to be sincere. Recruiting users and customers. I talk much more about this when we discuss the usability testing technique, but for now, be sure to talk primarily to people in your intended target market. You're looking for about an hour of their time. Location. It's always amazing to see customers in their native habitat. There's so much to learn just by observing their environment. But it's also fine to meet them somewhere convenient or have them come to your office. If you need to do this over a video call, that's not as good, but much better than not doing at all. Preparation. Be clear beforehand what problem it is you think they have, and think about how you'll either confirm or contradict that. Who should attend. My favorite is to bring three people to these interviews: the product manager, the product designer, and one of the engineers from the team (we normally rotate among those that want to attend). Usually, the designer drives (because they've usually been trained how to do this well), the product manager takes notes, and the developer observes. Interview. Work to keep things natural and informal, ask open‐ended questions, and try to learn what they're doing today (not so much what they wish they were doing, although that's also interesting). Afterward. Debrief with your colleagues to see if you've all heard the same things and had the same learnings. If you made any promises to the customer during that session, be sure you keep them. I would argue that this hour consistently yields a great return on your time. It's critical to learn the answers to these key questions. However, I am a big fan of taking the opportunity of a customer interview to also try out some of our product ideas. We do that after we've learned the answers to these key questions, but it's such a great opportunity I really like to take advantage of it.
                
> A concierge test is a relatively new name to describe an old but effective technique. The idea is that we do the customer's job for them—manually and personally. Just as if you went to a hotel concierge and asked if he could find you some theater tickets to a popular show. You don't really know the details of what that concierge is doing for you to get those tickets, but you do know that he is doing something.
                
> Historically, the two main approaches used by good teams to come up with product opportunities have been: Try to assess the market opportunities and pick potentially lucrative areas where significant pain exists. Look at what the technology or data enables—what's just now possible—and match that up with the significant pain.
                
> This third alternative is to allow, and even encourage, our customers to use our products to solve problems other than what we planned for and officially support.
                
> The two main types of hack days are directed and undirected. In an undirected hack day, people can explore whatever product ideas they like, so long as it's at least loosely related to the mission of the company.
                
> In a directed hack day, there's a customer problem (for example, something is really difficult to learn and use, or it takes too long to do) or business objective we've been assigned (for example, “Reduce the customer churn rate” or “Increase customer lifetime value”), and we ask people from the product teams to self‐organize and work on any ideas they like that might address this objective.
                
> There are two major benefits to these directed hack days. The first is practical, as the technique facilitates the inclusion of engineers at ideation. I've mentioned several times in this book that many of the best ideas come from the engineers on the team, and we need to ensure this is happening. It should be happening on an ongoing basis, but this technique will ensure it happens. The second benefit is cultural. This is one of my favorite techniques for building a team of missionaries rather than mercenaries.
                
> Prototypes of various forms have been around for as long as we've been applying technology to solve problems. Per the famous Fred Brooks quote, “Plan to throw one away; you will, anyhow.”
                
> That said, I continue to find teams, and even people I would consider thought leaders, that have a very narrow interpretation of what is meant by the term prototype. When I press people, what I typically find is that they associate the term prototype with the type that they were first exposed to. If the first one you saw was used to test for feasibility, that's what you think of. If the first one you saw was used for usability testing, that's what you think of. But there are in fact many very different forms of prototypes, each with different characteristics and each suited to testing different things.
                
> Feasibility Prototypes These are written by engineers to address technical feasibility risks during product discovery—before we decide whether something is feasible.
                
> User prototypes are simulations. There is a wide spectrum of user prototypes—from those intentionally designed to look like wireframes sketched out on paper (referred to as low‐fidelity user prototypes) all the way up to those that look and feel like the real thing (referred to as high‐fidelity user prototypes), where it can be difficult to tell it's just a simulation.
                
> Live‐data prototypes are a little more complicated to explain, but they are a critically important tool for several situations. The main purpose of a live‐data prototype is to collect actual data so we can prove something, or at least gather some evidence—normally to find out whether an idea (a feature, a design approach, a workflow) really works. This typically means two things. First, we need the prototype to access our live data sources, and second, we need to be able to send live traffic—in enough quantity to get some useful data—to the prototype.
                
> There are also many hybrids, which combine aspects of the other types. For example, when working on search and recommendations in which we're focusing on relevance, we may need to have the prototype access live‐data sources, but we don't need to be able to send live traffic. In this case, we're not trying to prove anything, but we can learn a great deal by observing and discussing the results with the users. Remember that product discovery is all about coming up with the fastest, cheapest way to test out our ideas. So, depending on your particular idea and situation, you'll want to pick the flavor of prototype that best meets your needs. While we all might have our favorites, if you are competing against good product teams, you are going to need to be skilled at each of these.
                
> But all forms of prototypes have certain characteristics and benefits in common. Here are five key principles behind their use. The overarching purpose of any form of prototype is to learn something at a much lower cost in terms of time and effort than building out a product. All forms of prototype should require at least an order of magnitude less time and effort as the eventual product. Realize that one of the key benefits of any form of prototype is to force you to think through a problem at a substantially deeper level than if we just talk about it or write something down. This is why the very act of creating a prototype so often exposes major issues otherwise left uncovered until much later. Similarly, a prototype is also a powerful tool for team collaboration. Members of the product team and business partners can all experience the prototype to develop shared understanding. The overarching purpose of any form of prototype is to learn something at a much lower cost in terms of time and effort than building out a product. There are many different possible levels of fidelity for a prototype. The fidelity primarily refers to how realistic the prototype looks. There is no such thing as one appropriate level of fidelity. Sometimes we don't need the prototype to look realistic at all, and other times it needs to be very realistic. The principle is that we create the right level of fidelity for its intended purpose, and we acknowledge that lower fidelity is faster and cheaper than higher fidelity, so we only do higher fidelity when we need to. The primary purpose of a prototype is to tackle one or more product risks (value, usability, feasibility, or viability) in discovery; however, in many cases, the prototype goes on to provide a second benefit, which is to communicate to the engineers and the broader organization what needs to be built. This is often referred to as prototype as spec. In many cases, the prototype is sufficient for this, but in other cases—especially when the engineers are not co‐located or when the product is especially complex—the prototype will likely need to be supplemented with additional details (usually, use cases, business rules, and acceptance criteria).
                
> The main technique used for tackling these types of risks is for one or more of the engineers to build a feasibility prototype. An engineer will create the feasibility prototype because it is typically code (as opposed to most prototypes created by special‐purpose tools intended to be used by product designers). A feasibility prototype is a long way from a commercially shippable product—the idea is to write just enough code to mitigate the feasibility risk. This typically represents just a small percentage of the work for the eventual shippable product. Further, most of the time the feasibility prototype is intended to be throwaway code—it's okay and normal to be quick and dirty with this. It is intended to be just enough to collect the data, for example, to show that performance would likely be acceptable or not. There is usually no user interface, error handling, or any of the typical work involved in productization.
                
> In my experience, building a feasibility prototype requires usually just a day or two of time.
                
> A user prototype—one of the most powerful tools in product discovery—is a simulation. Smoke and mirrors. It's all a façade. There is nothing behind the curtain. In other words, if you have a user prototype of an e‐commerce site, you can enter your credit card information as many times as you want—you won't actually be buying anything. There is a wide range of user prototypes.
                
> This is one of the most important techniques for product teams, so it is well worth developing your team's skills and experience in creating user prototypes at all levels of fidelity. As you'll see in the coming chapters, a user prototype is key to several types of validation and is also one of our most important communication tools.
                
> Sometimes, in order to address a major risk identified in discovery, we need to be able to collect some actual usage data. But we need to collect this evidence while in discovery, well before taking the time and expense of building an actual scalable and shippable product. Some of my favorite examples of this are when applying game dynamics, search result relevance, many social features, and product funnel work. This is the purpose of a live‐data prototype.
                
> There are two big limitations you do have to keep in mind, however: First, this is code, so engineers must create the live‐data prototype, not your designers. Second, this is not a commercially shippable product, it's not ready for primetime, and you can't run a business on it.
                
> One of my favorite examples of a hybrid prototype—and an exceptionally powerful tool for learning quickly in product discovery—is today often referred to as a Wizard of Oz prototype. A Wizard of Oz prototype combines the front‐end user experience of a high‐fidelity user prototype but with an actual person behind the scenes performing manually what would ultimately be handled by automation.
                
> In product discovery, we're essentially trying to quickly separate the good ideas from the bad as we work to try to solve the business problems assigned to us. But what does that really mean? We think of four types of questions we're trying to answer during discovery: Will the user or customer choose to use or buy this? (Value) Can the user figure out how to use this? (Usability) Can we build this? (Feasibility) Is this solution viable for our business? (Business viability)
                
> The main activity of discovery is when these answers are not so clear. There is no prescribed order to answering these questions. However, many teams follow a certain logic. First, we will usually assess value. This is often the toughest—and most important—question to answer, and if the value isn't there, not much else matters. We likely will need to address usability before the user or customer can even recognize the value. In either case, we usually assess usability and value with the same users and customers at the same time.
                
> Once we have something that our customers believe is truly valuable, and we have designed it in a way that we believe our users can figure out how to use, then we'll typically review the approach with the engineers to make sure this is doable from their technical feasibility perspective. If we're also good on feasibility, then we'll show it to key parts of the business where there may be concerns (think legal, marketing, sales, CEO, etc.). We'll often address these business risks last because we don't want to stir up the organization unless we're confident it's worthwhile.
                
> Customers don't have to buy our products, and users don't have to choose to use a feature. They will only do so if they perceive real value. Another way to think about this is that just because someone can use our product doesn't mean they will choose to use our product.
                
> The customer must perceive your product to be substantially better to motivate them to buy your product and then wade through the pain and obstacles of migrating from their old solution.
                
> All of this is a long way of saying that good product teams spend most of their time on creating value. If the value is there, we can fix everything else. If it's not, how good our usability, reliability, or performance is doesn't matter.
                
> Sometimes it's unclear if there's demand for what we want to build.
                
> The most common type of qualitative value testing is focused on the response, or reaction. Do customers love this? Will they pay for it? Will users choose to use this? And most important, if not, why not?
                
> For many products, we need to test efficacy, which refers to how well this solution solves the underlying problem. In some types of products, this is very objective and quantitative.
                
> One of the biggest possible wastes of time and effort, and the reason for countless failed startups, is when a team designs and builds a product—testing usability, testing reliability, testing performance, and doing everything they think they're supposed to do—yet, when they finally release the product, they find that people won't buy it.
                
> The demand‐testing technique is called a fake door demand test. The idea is that we put the button or menu item into the user experience exactly where we believe it should be. But, when the user clicks that button, rather than taking the user to the new feature, it instead takes the user to a special page that explains that you are studying the possibility of adding this new feature, and you are seeking customers to talk to about this. The page also provides a way for the user to volunteer (by providing their e‐mail or phone number, for example).
                
> The same basic concept applies to entire products. Rather than a button on a page, we set up the landing page for the new offering's product funnel. This is called a landing page demand test. We describe that new offering exactly as we would if we were really launching the service. The difference is that if the user clicks the call to action, rather than signing up for the trial (or whatever the action might be), the user sees a page that explains that you are studying the possibility of adding this new offering, and you'd like to talk with them about that new offering if they're willing.
                
> Hopefully, you can see that this is very easy to do, and you can quickly collect two very useful things: (1) some good evidence on demand and (2) a list of users who are very ready and willing to talk with you about this specific new capability.
                
> Quantitative testing tells us what's happening (or not), but it can't tell us why, and what to do to correct the situation. That's why we do qualitative testing. If users and customers are not responding to a product the way we had hoped, we need to figure out why that's the case. As a reminder, qualitative testing is not about proving anything. That's what quantitative testing is for. Qualitative testing is about rapid learning and big insights.
                
> When you do this type of qualitative user testing, you don't get your answer from any one user, but every user you test with is like another piece of the puzzle. Eventually, you see enough of the puzzle that you can understand where you've gone wrong. I know this is a big claim, but I argue that qualitative testing of your product ideas with real users and customers is probably the single most important discovery activity for you and your product team. It is so important and helpful that I push product teams to do at least two or three qualitative value tests every single week. Here's how to do it:
                
> We generally begin the user test with a short user interview where we try to make sure our user has the problems we think she has, how she solves these problems today, and what it would take for her to switch (see Customer Interview Technique).
                
> During the usability test, we test to see whether the user can figure out how to operate our product. But, even more important, after a usability test the user knows what your product is all about and how it's meant to be used. Only then can we have a useful conversation with the user about value (or lack thereof). Preparing a value test therefore includes preparing a usability test.
                
> To test usability and value, the user needs to be able to use one of the prototypes we described earlier. When we're focused on testing value, we usually utilize high‐fidelity user prototypes.
                
> One technique I like for gauging value is to see if the user would be willing to pay for it, even if you have no intention of charging them for it. We're looking for the user to pull out his or her credit card right then and there and ask to buy the product (but we don't really want the card information). If it's an expensive product for businesses—beyond what someone would put on a credit card—you can ask people if they will sign a “non‐binding letter of intent to buy” which is a good indicator that people are serious.
                
> But there are other ways a user can “pay” for a product. You can see if they would be willing to pay with their reputation. You can ask them how likely they'd be to recommend the product to their friends or co‐workers or boss (typically on a scale of 0–10).
                
> Especially with businesses, you can also ask the person if they'd be willing to schedule some significant time with you to work on this (even if we don't need it). This is another way people pay for value.
                
> You can also ask people to provide the login credentials for whatever product they would be switching from (because you tell them there's a migration utility or something). Again, we don't really want their login and password—we just want to know if they value our product highly enough that they're truly willing to switch right then and there.
                
> Remember, this is not about proving anything. It's about rapid learning. As soon as you believe you have a problem, or you want to try a different approach, you try it. For example, if you show your prototype to two different people and the response you get is substantially different, your job is to try to figure out why.
                
> The remarkable thing about this kind of qualitative testing is just how easy and effective it is. The best way to prove this to yourself is to take your laptop or mobile device with your product or prototype on it to someone who hasn't seen it yet, and just give it a try.
                
> One important note. As product manager, you need to make sure you are at every single qualitative value test. Do not delegate this, and certainly don't try to hire a firm to do this for you. Your contribution to the team comes from experiencing as many users as possible, first hand, interacting with and responding to your team's ideas. If you worked for me, the continuation of your monthly salary would depend on this.
                
> A discovery sprint is a one‐week time box of product discovery work, designed to tackle a substantial problem or risk your product team is facing.
                
> A discovery sprint is definitely useful for more than just transformation. It could just as easily be considered a discovery planning technique or a discovery prototyping technique. But I find it's most helpful in bringing all these things together, so I choose to include it here.
                
> Some people use the term design sprint rather than discovery sprint, but as the purpose of the work—when done well—goes significantly beyond design, I prefer the more general term. Further, if your company has been struggling with the concept of MVP, this is a very good way to start getting the value from this key technique.
                
> In any case, during this week of intense discovery work, you and your team will likely explore dozens of different product ideas and approaches, with the goal of solving some significant business problem. You'll always end your week by validating your potential solution with real users and customers.
                
> The book is titled, Sprint: How to Solve Big Problems and Test New Ideas in Just Five Days, by Jake Knapp, John Zeratsky, and Braden Kowitz.
                
> Rather than fight this reality, we can embrace it. One of the simplest techniques for facilitating moving to new ways of working is the use of pilot teams. Pilot teams allow the roll out of change to a limited part of the organization before implementing it more broadly. The idea is that you look for a product team to volunteer to try out some new techniques. You let them run for a while (usually a quarter or two) with this new way of working and see how this goes.
                
> For this to work, it's important to acknowledge the two big reasons why stakeholders especially are so attracted to roadmaps: They want some visibility into what you are working on and assurance that you are working on the most important items. They want to be able to plan the business and need to know when critical things will happen. The modern alternative to roadmaps discussed here addresses both of these concerns. Teams work on the prioritized business objectives determined by the leaders; we share our key results transparently; and we commit to high‐integrity commitments when critical delivery dates are needed.
                
> With a grateful nod to Ben Horowitz's classic post “Good Product Manager/Bad Product Manager,” for those that have not yet had the opportunity to participate in or observe a strong product team up close, in this chapter I provide you with a glimpse into some of the important differences between strong product teams and weak teams: Good teams have a compelling product vision that they pursue with a missionary‐like passion. Bad teams are mercenaries. Good teams get their inspiration and product ideas from their vision and objectives, from observing customers' struggle, from analyzing the data customers generate from using their product, and from constantly seeking to apply new technology to solve real problems. Bad teams gather requirements from sales and customers. Good teams understand who each of their key stakeholders are, they understand the constraints that these stakeholders operate in, and they are committed to inventing solutions that work not just for users and customers, but also work within the constraints of the business. Bad teams gather requirements from stakeholders. Good teams are skilled in the many techniques to rapidly try out product ideas to determine which ones are truly worth building. Bad teams hold meetings to generate prioritized roadmaps. Good teams love to have brainstorming discussions with smart thought leaders from across the company. Bad teams get offended when someone outside their team dares to suggest they do something. Good teams have product, design, and engineering sit side by side, and they embrace the give and take between the functionality, the user experience, and the enabling technology. Bad teams sit in their respective silos, and ask that others make requests for their services in the form of documents and scheduling meetings. Good teams are constantly trying out new ideas to innovate, but doing so in ways that protect the revenue and protect the brand. Bad teams are still waiting for permission to run a test. Good teams insist they have the skill sets on their team, such as strong product design, necessary to create winning products. Bad teams don't even know what product designers are. Good teams ensure that their engineers have time to try out the prototypes in discovery every day so that they can contribute their thoughts on how to make the product better. Bad teams show the prototypes to the engineers during sprint planning so they can estimate. Good teams engage directly with end users and customers every week, to better understand their customers, and to see the customer's response to their latest ideas. Bad teams think they are the customer. Good teams know that many of their favorite ideas won't end up working for customers, and even the ones that could will need several iterations to get to the point where they provide the desired outcome. Bad teams just build what's on the roadmap, and are satisfied with meeting dates and ensuring quality. Good teams understand the need for speed and how…

> What does it really mean to have a strong innovation culture? Culture of experimentation—teams know they can run tests; some will succeed and many will fail, and this is acceptable and understood. Culture of open minds—teams know that good ideas can come from anywhere and aren't always obvious at the outset. Culture of empowerment—individuals and teams feel empowered to be able to try out an idea. Culture of technology—teams realize that true innovation can be inspired by new technology and analysis of data, as well as by customers. Culture of business‐ and customer‐savvy teams—teams, including developers, have a deep understanding of the business needs and constraints, and understanding of (and access to) the users and customers. Culture of skill‐set and staff diversity—teams appreciate that different skills and backgrounds contribute to innovative solutions—especially engineering, design, and product. Culture of discovery techniques—the mechanisms are in place for ideas to be tested out quickly and safely (protecting brand, revenue, customers, and colleagues). What does it really mean to have a strong execution culture? Culture of urgency—people feel like they are in wartime, and that if they don't find a way to move fast, then bad things could happen. Culture of high‐integrity commitments—teams understand the need for (and power of) commitments, but they also insist on high‐integrity commitments. Culture of empowerment—teams feel as though they have the tools, resources, and permission to do whatever is necessary to meet their commitments. Culture of accountability—people and teams feel a deep responsibility to meet their commitments. Accountability also implies consequences—not necessarily being terminated, except in extreme and repeated situations, but more likely consequences to their reputations among their peers. Culture of collaboration—while team autonomy and empowerment is important, teams understand their even higher need to work together to accomplish many of the biggest and most meaningful objectives. Culture of results—is the focus on output or is the focus on results? Culture of recognition—teams often take their cues from what is rewarded and what is accepted. Is it just the team that comes up with the great new idea that gets rewarded, or the team that delivered on a brutally tough commitment? And what is the message if missing a commitment is seen as easily excusable? So, if these characteristics help define each culture, this begs some pretty tough questions: Is an innovation culture in any way inherently at odds with an execution culture? Does a strong execution culture lead to a stressful (or worse) work environment? What types of people, including leaders, are attracted to, and needed, for each type of culture? I can tell you that there do exist companies that are very strong at both consistent innovation and execution. Amazon is one of the best examples.